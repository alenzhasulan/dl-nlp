{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjp+26Of+uqWdpqFaTyZ/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alenzhasulan/dl-nlp/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt3LeHxpBXwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYwDVTWvIhzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Загружаем данные\n",
        "JSONFile = open('data.json')\n",
        "data = json.loads(JSONFile.read())\n",
        "JSONFile.close()\n",
        "\n",
        "\n",
        "train_source=data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkM49ltvFQb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOKEN_RE=re.compile(r'[\\w\\d]+')\n",
        "\n",
        "#Возвращает для каждого строку \n",
        "def tokenize_text_regex(txt,min_token_size=3):\n",
        "  txt=txt.lower()\n",
        "  all_tokens=TOKEN_RE.findall(txt)\n",
        "  return [token for token in all_tokens if len(token)>=min_token_size]\n",
        "\n",
        "#Для токенезаций\n",
        "def tokenize_corpus(texts,tokenizer=tokenize_text_regex,**tokenizer_kwargs):\n",
        "  return [tokenizer(text,**tokenizer_kwargs) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCet8LVSKgFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokinezed_corpus=tokenize_corpus(train_source)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6ZPa6RKNCNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(tokenized_texts,max_size=10000000,max_doc_freq=0.8,min_count=5,pad_word=None):\n",
        "  word_counts=collections.defaultdict(int)\n",
        "  doc_n=0\n",
        "\n",
        "  # подсчитать количество документов, в которых употребляется слова каждое слово\n",
        "  # а также общее количество документов\n",
        "  for txt in tokenized_texts:\n",
        "    doc_n+=1\n",
        "    unique_text_tokens=set(txt)\n",
        "    for token in unique_text_tokens:\n",
        "      word_counts[token]+=1\n",
        "  \n",
        "  # убрать слишком редкие и слишком частые слова\n",
        "  word_counts={word:cnt for word,cnt in word_counts.items() if cnt>=min_count and cnt/doc_n<=max_doc_freq}\n",
        "\n",
        "  # отсортировать слова по убыванию частоте \n",
        "  sorted_word_counts=sorted(word_counts.items(),reverse=True,key=lambda pair:pair[1])\n",
        "\n",
        "  #добавим несуществующие слова с индексом 0 для удобство пакетной отроботки\n",
        "  if pad_word is not None:\n",
        "    sorted_word_counts=[(pad_word,0)]+sorted_word_counts\n",
        "\n",
        "  #если у нас много слов от оставить самых частотных\n",
        "  if len(word_counts)>max_size:\n",
        "    sorted_word_counts=sorted_word_counts[:max_size]\n",
        "\n",
        "  #нумируем слова\n",
        "  word2id={word:i for i ,(word,_) in enumerate(sorted_word_counts)}\n",
        "\n",
        "  #нормируем частотных слов\n",
        "  word2freq=np.array([cnt/doc_n for _,cnt in sorted_word_counts],dtype='float32')\n",
        "\n",
        "  return word2id,word2freq\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_7zOHvYJJRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DF=0.8\n",
        "MIN_COUNT=5\n",
        "\n",
        "vocabulary, word_doc_freq=build_vocabulary(tokinezed_corpus,\n",
        "                                           max_doc_freq=MAX_DF,\n",
        "                                           min_count=5\n",
        "                                           )\n",
        "UNIQUE_WORDS_N=len(vocabulary)\n",
        "# print('Количество уникальных токенов ',UNIQUE_WORDS_N)\n",
        "# print(list(vocabulary.items())[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}