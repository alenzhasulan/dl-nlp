{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Копия блокнота \"nmt_with_attention.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alenzhasulan/dl-nlp/blob/master/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22nmt_with_attention_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from string import digits"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcMBc0_Yos-d"
      },
      "source": [
        "data_path = \"mfa_gov_kz_corpus.tsv\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY1T_WcDpfqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ee65a006-3532-47ca-8cc6-c9c15fb2dc5f"
      },
      "source": [
        "df = pd.read_csv('mfa_gov_kz_corpus.tsv',sep='\\t', error_bad_lines=False,names=['source','target', 'coef'])\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>лисаббон .</td>\n",
              "      <td>lisbon .</td>\n",
              "      <td>1.225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>сайтта инвестициялауды қажет ететін жобалар ту...</td>\n",
              "      <td>visitors can also send requests through the si...</td>\n",
              "      <td>0,04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>тараптар осы жылдың екінші жартысында еқыұ мен...</td>\n",
              "      <td>the first president offered a new \" 3d \" geopo...</td>\n",
              "      <td>0,09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>өз кезегінде \" kazakh invest \" ұк \" ақ басқарм...</td>\n",
              "      <td>volume of investments in the region has been g...</td>\n",
              "      <td>0,13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>анықтама :</td>\n",
              "      <td>note to editors .</td>\n",
              "      <td>0,15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              source  ...   coef\n",
              "0                                         лисаббон .  ...  1.225\n",
              "1  сайтта инвестициялауды қажет ететін жобалар ту...  ...   0,04\n",
              "2  тараптар осы жылдың екінші жартысында еқыұ мен...  ...   0,09\n",
              "3  өз кезегінде \" kazakh invest \" ұк \" ақ басқарм...  ...   0,13\n",
              "4                                         анықтама :  ...   0,15\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BobwqO3-os-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "83c05d70-a421-49d7-e0e6-6be651fa9680"
      },
      "source": [
        "#Read the data\n",
        "lines_raw= pd.read_csv('mfa_gov_kz_corpus.tsv',sep='\\t', error_bad_lines=False,names=['source','target', 'coef'])\n",
        "lines_raw.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6536</th>\n",
              "      <td>астана , 2018 жылғы 20 қараша астана , 2018 жы...</td>\n",
              "      <td>astana , november 20 , 2018 - kazakhstan-russi...</td>\n",
              "      <td>1,44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4356</th>\n",
              "      <td>kazakh invest  ұлттық компаниясы \" ақ қр сырт...</td>\n",
              "      <td>kazakh invest is a subordinate organization of...</td>\n",
              "      <td>1,05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5988</th>\n",
              "      <td>бұл іс-шараның қазақстан үкіметінің гендерлік...</td>\n",
              "      <td>1,30</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2994</th>\n",
              "      <td>қ . әбдірахманов халықаралық аренада қазіргі б...</td>\n",
              "      <td>kairat abdrakhmanov emphasized the role of the...</td>\n",
              "      <td>0,90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7784</th>\n",
              "      <td>қазақстан 2014 жылдың қараша айынан бастап 1 м...</td>\n",
              "      <td>kazakhstan since november 2014 is an observer ...</td>\n",
              "      <td>2,29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 source  ...  coef\n",
              "6536  астана , 2018 жылғы 20 қараша астана , 2018 жы...  ...  1,44\n",
              "4356   kazakh invest  ұлттық компаниясы \" ақ қр сырт...  ...  1,05\n",
              "5988   бұл іс-шараның қазақстан үкіметінің гендерлік...  ...   NaN\n",
              "2994  қ . әбдірахманов халықаралық аренада қазіргі б...  ...  0,90\n",
              "7784  қазақстан 2014 жылдың қараша айынан бастап 1 м...  ...  2,29\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK7arZyYtu2n"
      },
      "source": [
        "kaz= df['source'][:100].str.lower()\n",
        "en= df['target'][:100].str.lower()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTbSbBz55QtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53237b1-00fa-4a95-bff9-dcbde38e9550"
      },
      "source": [
        "sample_size=60000\n",
        "source='start_ ' + kaz + ' _end'\n",
        "target ='start_ ' + en + ' _end'\n",
        "source"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                start_ лисаббон . _end\n",
              "1     start_ сайтта инвестициялауды қажет ететін жоб...\n",
              "2     start_ тараптар осы жылдың екінші жартысында е...\n",
              "3     start_ өз кезегінде \" kazakh invest \" ұк \" ақ ...\n",
              "4                                start_ анықтама : _end\n",
              "                            ...                        \n",
              "95    start_ қазіргі таңда қазақстандық бидайды мысы...\n",
              "96    start_ конференция барысында халықаралық терро...\n",
              "97    start_ аустрияның \" öwd wachdienst security gm...\n",
              "98    start_ мысалы , жапон орта мектептерінде дрк т...\n",
              "99    start_ қ . абдрахманов \" төмен байытылған уран...\n",
              "Name: source, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmMZQpdO60dt"
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wMBDTI4os-g"
      },
      "source": [
        "__Create the source and ttarget toekns and post pad them__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZoRYxcos-h"
      },
      "source": [
        "source_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_sentence_tokenizer.fit_on_texts(source)\n",
        "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
        "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,padding='post' )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sblN_oPhos-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38671d8d-208a-4583-e852-2edc9d04b076"
      },
      "source": [
        "target_sentence_tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_sentence_tokenizer.fit_on_texts(target)\n",
        "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
        "target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor,padding='post' )\n",
        "print(len(target_tensor[0]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SfoO6LTos-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac5036c-a09c-4b08-c825-f13e718f6135"
      },
      "source": [
        "max_target_length= max(len(t) for t in  target_tensor)\n",
        "print(max_target_length)\n",
        "max_source_length= max(len(t) for t in  source_tensor)\n",
        "print(max_source_length)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58\n",
            "79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7MX_rG-os-i"
      },
      "source": [
        "##  Creating Train and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67gSPANlos-i"
      },
      "source": [
        "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor= train_test_split(source_tensor, target_tensor,test_size=0.2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045f79e0-363c-4cf3-adc7-ad6986ee84fb"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80 80 20 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVNTq43yos-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2c999f-9858-4943-9253-6b3653cdf592"
      },
      "source": [
        "type(input_tensor_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXukARTDd7MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df0e659-0852-47dc-b905-f1c4a541ddb9"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(source_sentence_tokenizer, source_train_tensor[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert( target_sentence_tokenizer, target_train_tensor[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> start_\n",
            "878 ----> талқылаудың\n",
            "94 ----> маңызды\n",
            "879 ----> тақырыптарының\n",
            "96 ----> бірі\n",
            "880 ----> нұр-сұлтанда\n",
            "38 ----> еқыұ\n",
            "881 ----> тақырыптық\n",
            "882 ----> орталығын\n",
            "31 ----> құру\n",
            "9 ----> туралы\n",
            "13 ----> қазақстандық\n",
            "883 ----> бастама\n",
            "203 ----> болды\n",
            "3 ----> .\n",
            "2 ----> _end\n",
            "\n",
            "Target Language; index to word mapping\n",
            "3 ----> start_\n",
            "26 ----> one\n",
            "2 ----> of\n",
            "1 ----> the\n",
            "58 ----> main\n",
            "114 ----> topics\n",
            "2 ----> of\n",
            "218 ----> discussion\n",
            "12 ----> was\n",
            "1 ----> the\n",
            "14 ----> kazakh\n",
            "612 ----> initiative\n",
            "9 ----> to\n",
            "613 ----> establish\n",
            "45 ----> an\n",
            "37 ----> osce\n",
            "614 ----> thematic\n",
            "71 ----> center\n",
            "7 ----> in\n",
            "98 ----> nur-sultan\n",
            "15 ----> on\n",
            "615 ----> sustainable\n",
            "616 ----> interconnection\n",
            "6 ----> ,\n",
            "617 ----> focused\n",
            "15 ----> on\n",
            "618 ----> analytic\n",
            "8 ----> and\n",
            "619 ----> research\n",
            "219 ----> work\n",
            "5 ----> .\n",
            "4 ----> _end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046e1e03-1456-4010-9126-c24aa189bf0f"
      },
      "source": [
        "BUFFER_SIZE = len(source_train_tensor)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(source_train_tensor)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "type(dataset)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.BatchDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac04fdec-62f1-4ea2-e456-7deea9192654"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 79]), TensorShape([64, 58]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea602e14-e8c6-4a2c-c281-28f4c94ce03c"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 79, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umohpBN2OM94"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k534zTHiDjQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e51109b-7163-4d9c-c2d7-b4822cd4e37d"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 79, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35031d6e-e65d-4e41-f824-cb76906143d8"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 854)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXhSFaOios-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebfbc73-3b5f-44b2-8c80-64ab904d14a2"
      },
      "source": [
        "steps_per_epoch"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6484e830-dc07-4be2-95df-620f641f186b"
      },
      "source": [
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} loss {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
        "   \n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 loss 2.5616629123687744\n",
            "Epoch 1 Loss 2.5617\n",
            "Time taken for 1 epoch 0.8157966136932373 sec\n",
            "\n",
            "Epoch 2 Batch 0 loss 2.506190061569214\n",
            "Epoch 2 Loss 2.5062\n",
            "Time taken for 1 epoch 1.0423014163970947 sec\n",
            "\n",
            "Epoch 3 Batch 0 loss 2.5563480854034424\n",
            "Epoch 3 Loss 2.5563\n",
            "Time taken for 1 epoch 0.7663540840148926 sec\n",
            "\n",
            "Epoch 4 Batch 0 loss 2.4221394062042236\n",
            "Epoch 4 Loss 2.4221\n",
            "Time taken for 1 epoch 1.0684523582458496 sec\n",
            "\n",
            "Epoch 5 Batch 0 loss 2.3820948600769043\n",
            "Epoch 5 Loss 2.3821\n",
            "Time taken for 1 epoch 0.7500848770141602 sec\n",
            "\n",
            "Epoch 6 Batch 0 loss 2.375213384628296\n",
            "Epoch 6 Loss 2.3752\n",
            "Time taken for 1 epoch 1.0652902126312256 sec\n",
            "\n",
            "Epoch 7 Batch 0 loss 2.320357322692871\n",
            "Epoch 7 Loss 2.3204\n",
            "Time taken for 1 epoch 0.7514598369598389 sec\n",
            "\n",
            "Epoch 8 Batch 0 loss 2.2855160236358643\n",
            "Epoch 8 Loss 2.2855\n",
            "Time taken for 1 epoch 1.0525424480438232 sec\n",
            "\n",
            "Epoch 9 Batch 0 loss 2.2555060386657715\n",
            "Epoch 9 Loss 2.2555\n",
            "Time taken for 1 epoch 0.7754132747650146 sec\n",
            "\n",
            "Epoch 10 Batch 0 loss 2.0517735481262207\n",
            "Epoch 10 Loss 2.0518\n",
            "Time taken for 1 epoch 1.0683882236480713 sec\n",
            "\n",
            "Epoch 11 Batch 0 loss 2.284146785736084\n",
            "Epoch 11 Loss 2.2841\n",
            "Time taken for 1 epoch 0.7514641284942627 sec\n",
            "\n",
            "Epoch 12 Batch 0 loss 1.957279086112976\n",
            "Epoch 12 Loss 1.9573\n",
            "Time taken for 1 epoch 1.076298475265503 sec\n",
            "\n",
            "Epoch 13 Batch 0 loss 2.0470504760742188\n",
            "Epoch 13 Loss 2.0471\n",
            "Time taken for 1 epoch 0.755826473236084 sec\n",
            "\n",
            "Epoch 14 Batch 0 loss 1.8840234279632568\n",
            "Epoch 14 Loss 1.8840\n",
            "Time taken for 1 epoch 1.0910847187042236 sec\n",
            "\n",
            "Epoch 15 Batch 0 loss 1.9663951396942139\n",
            "Epoch 15 Loss 1.9664\n",
            "Time taken for 1 epoch 0.7498927116394043 sec\n",
            "\n",
            "Epoch 16 Batch 0 loss 2.0534651279449463\n",
            "Epoch 16 Loss 2.0535\n",
            "Time taken for 1 epoch 1.1136806011199951 sec\n",
            "\n",
            "Epoch 17 Batch 0 loss 2.051281452178955\n",
            "Epoch 17 Loss 2.0513\n",
            "Time taken for 1 epoch 0.753040075302124 sec\n",
            "\n",
            "Epoch 18 Batch 0 loss 2.042318105697632\n",
            "Epoch 18 Loss 2.0423\n",
            "Time taken for 1 epoch 1.082348108291626 sec\n",
            "\n",
            "Epoch 19 Batch 0 loss 1.9946832656860352\n",
            "Epoch 19 Loss 1.9947\n",
            "Time taken for 1 epoch 0.7520051002502441 sec\n",
            "\n",
            "Epoch 20 Batch 0 loss 1.9263602495193481\n",
            "Epoch 20 Loss 1.9264\n",
            "Time taken for 1 epoch 1.0823054313659668 sec\n",
            "\n",
            "Epoch 21 Batch 0 loss 1.912966012954712\n",
            "Epoch 21 Loss 1.9130\n",
            "Time taken for 1 epoch 0.7502031326293945 sec\n",
            "\n",
            "Epoch 22 Batch 0 loss 2.0495145320892334\n",
            "Epoch 22 Loss 2.0495\n",
            "Time taken for 1 epoch 1.0816569328308105 sec\n",
            "\n",
            "Epoch 23 Batch 0 loss 1.8858814239501953\n",
            "Epoch 23 Loss 1.8859\n",
            "Time taken for 1 epoch 0.7581095695495605 sec\n",
            "\n",
            "Epoch 24 Batch 0 loss 1.8997700214385986\n",
            "Epoch 24 Loss 1.8998\n",
            "Time taken for 1 epoch 1.0634222030639648 sec\n",
            "\n",
            "Epoch 25 Batch 0 loss 1.8547457456588745\n",
            "Epoch 25 Loss 1.8547\n",
            "Time taken for 1 epoch 0.7746427059173584 sec\n",
            "\n",
            "Epoch 26 Batch 0 loss 1.9069026708602905\n",
            "Epoch 26 Loss 1.9069\n",
            "Time taken for 1 epoch 1.075227975845337 sec\n",
            "\n",
            "Epoch 27 Batch 0 loss 1.92530357837677\n",
            "Epoch 27 Loss 1.9253\n",
            "Time taken for 1 epoch 0.7805683612823486 sec\n",
            "\n",
            "Epoch 28 Batch 0 loss 1.800051212310791\n",
            "Epoch 28 Loss 1.8001\n",
            "Time taken for 1 epoch 1.1121635437011719 sec\n",
            "\n",
            "Epoch 29 Batch 0 loss 1.917360782623291\n",
            "Epoch 29 Loss 1.9174\n",
            "Time taken for 1 epoch 0.755134105682373 sec\n",
            "\n",
            "Epoch 30 Batch 0 loss 1.9091297388076782\n",
            "Epoch 30 Loss 1.9091\n",
            "Time taken for 1 epoch 1.1261370182037354 sec\n",
            "\n",
            "Epoch 31 Batch 0 loss 1.844725489616394\n",
            "Epoch 31 Loss 1.8447\n",
            "Time taken for 1 epoch 0.7610795497894287 sec\n",
            "\n",
            "Epoch 32 Batch 0 loss 1.8479502201080322\n",
            "Epoch 32 Loss 1.8480\n",
            "Time taken for 1 epoch 1.950571060180664 sec\n",
            "\n",
            "Epoch 33 Batch 0 loss 1.7436010837554932\n",
            "Epoch 33 Loss 1.7436\n",
            "Time taken for 1 epoch 0.7673585414886475 sec\n",
            "\n",
            "Epoch 34 Batch 0 loss 1.7646868228912354\n",
            "Epoch 34 Loss 1.7647\n",
            "Time taken for 1 epoch 2.691197395324707 sec\n",
            "\n",
            "Epoch 35 Batch 0 loss 1.773275375366211\n",
            "Epoch 35 Loss 1.7733\n",
            "Time taken for 1 epoch 0.8710522651672363 sec\n",
            "\n",
            "Epoch 36 Batch 0 loss 1.6603895425796509\n",
            "Epoch 36 Loss 1.6604\n",
            "Time taken for 1 epoch 3.0955331325531006 sec\n",
            "\n",
            "Epoch 37 Batch 0 loss 1.7051810026168823\n",
            "Epoch 37 Loss 1.7052\n",
            "Time taken for 1 epoch 0.82696533203125 sec\n",
            "\n",
            "Epoch 38 Batch 0 loss 1.7381870746612549\n",
            "Epoch 38 Loss 1.7382\n",
            "Time taken for 1 epoch 6.724609136581421 sec\n",
            "\n",
            "Epoch 39 Batch 0 loss 1.7367448806762695\n",
            "Epoch 39 Loss 1.7367\n",
            "Time taken for 1 epoch 0.8513882160186768 sec\n",
            "\n",
            "Epoch 40 Batch 0 loss 1.7295291423797607\n",
            "Epoch 40 Loss 1.7295\n",
            "Time taken for 1 epoch 1.1395819187164307 sec\n",
            "\n",
            "Epoch 41 Batch 0 loss 1.6979273557662964\n",
            "Epoch 41 Loss 1.6979\n",
            "Time taken for 1 epoch 0.762845516204834 sec\n",
            "\n",
            "Epoch 42 Batch 0 loss 1.694905400276184\n",
            "Epoch 42 Loss 1.6949\n",
            "Time taken for 1 epoch 1.1544280052185059 sec\n",
            "\n",
            "Epoch 43 Batch 0 loss 1.582453966140747\n",
            "Epoch 43 Loss 1.5825\n",
            "Time taken for 1 epoch 0.7667326927185059 sec\n",
            "\n",
            "Epoch 44 Batch 0 loss 1.5646284818649292\n",
            "Epoch 44 Loss 1.5646\n",
            "Time taken for 1 epoch 2.588735342025757 sec\n",
            "\n",
            "Epoch 45 Batch 0 loss 1.586192011833191\n",
            "Epoch 45 Loss 1.5862\n",
            "Time taken for 1 epoch 0.7984418869018555 sec\n",
            "\n",
            "Epoch 46 Batch 0 loss 1.5112379789352417\n",
            "Epoch 46 Loss 1.5112\n",
            "Time taken for 1 epoch 3.724210262298584 sec\n",
            "\n",
            "Epoch 47 Batch 0 loss 1.5539511442184448\n",
            "Epoch 47 Loss 1.5540\n",
            "Time taken for 1 epoch 0.8263123035430908 sec\n",
            "\n",
            "Epoch 48 Batch 0 loss 1.5110478401184082\n",
            "Epoch 48 Loss 1.5110\n",
            "Time taken for 1 epoch 6.019557237625122 sec\n",
            "\n",
            "Epoch 49 Batch 0 loss 1.546107530593872\n",
            "Epoch 49 Loss 1.5461\n",
            "Time taken for 1 epoch 0.830909013748169 sec\n",
            "\n",
            "Epoch 50 Batch 0 loss 1.462573766708374\n",
            "Epoch 50 Loss 1.4626\n",
            "Time taken for 1 epoch 1.1008281707763672 sec\n",
            "\n",
            "Epoch 51 Batch 0 loss 1.4616353511810303\n",
            "Epoch 51 Loss 1.4616\n",
            "Time taken for 1 epoch 0.7643969058990479 sec\n",
            "\n",
            "Epoch 52 Batch 0 loss 1.3643218278884888\n",
            "Epoch 52 Loss 1.3643\n",
            "Time taken for 1 epoch 1.4007902145385742 sec\n",
            "\n",
            "Epoch 53 Batch 0 loss 1.3579076528549194\n",
            "Epoch 53 Loss 1.3579\n",
            "Time taken for 1 epoch 0.7647733688354492 sec\n",
            "\n",
            "Epoch 54 Batch 0 loss 1.3637415170669556\n",
            "Epoch 54 Loss 1.3637\n",
            "Time taken for 1 epoch 3.8604483604431152 sec\n",
            "\n",
            "Epoch 55 Batch 0 loss 1.3752009868621826\n",
            "Epoch 55 Loss 1.3752\n",
            "Time taken for 1 epoch 0.860933780670166 sec\n",
            "\n",
            "Epoch 56 Batch 0 loss 1.396531581878662\n",
            "Epoch 56 Loss 1.3965\n",
            "Time taken for 1 epoch 1.8743445873260498 sec\n",
            "\n",
            "Epoch 57 Batch 0 loss 1.38645339012146\n",
            "Epoch 57 Loss 1.3865\n",
            "Time taken for 1 epoch 0.7734825611114502 sec\n",
            "\n",
            "Epoch 58 Batch 0 loss 1.330519437789917\n",
            "Epoch 58 Loss 1.3305\n",
            "Time taken for 1 epoch 5.28223729133606 sec\n",
            "\n",
            "Epoch 59 Batch 0 loss 1.3978271484375\n",
            "Epoch 59 Loss 1.3978\n",
            "Time taken for 1 epoch 0.8510854244232178 sec\n",
            "\n",
            "Epoch 60 Batch 0 loss 1.2928040027618408\n",
            "Epoch 60 Loss 1.2928\n",
            "Time taken for 1 epoch 1.167095422744751 sec\n",
            "\n",
            "Epoch 61 Batch 0 loss 1.3354653120040894\n",
            "Epoch 61 Loss 1.3355\n",
            "Time taken for 1 epoch 0.7656431198120117 sec\n",
            "\n",
            "Epoch 62 Batch 0 loss 1.3145298957824707\n",
            "Epoch 62 Loss 1.3145\n",
            "Time taken for 1 epoch 2.0978779792785645 sec\n",
            "\n",
            "Epoch 63 Batch 0 loss 1.193111777305603\n",
            "Epoch 63 Loss 1.1931\n",
            "Time taken for 1 epoch 0.7628352642059326 sec\n",
            "\n",
            "Epoch 64 Batch 0 loss 1.2714133262634277\n",
            "Epoch 64 Loss 1.2714\n",
            "Time taken for 1 epoch 3.734046459197998 sec\n",
            "\n",
            "Epoch 65 Batch 0 loss 1.1388715505599976\n",
            "Epoch 65 Loss 1.1389\n",
            "Time taken for 1 epoch 0.8530447483062744 sec\n",
            "\n",
            "Epoch 66 Batch 0 loss 1.2116256952285767\n",
            "Epoch 66 Loss 1.2116\n",
            "Time taken for 1 epoch 2.4929356575012207 sec\n",
            "\n",
            "Epoch 67 Batch 0 loss 1.2490750551223755\n",
            "Epoch 67 Loss 1.2491\n",
            "Time taken for 1 epoch 0.7770609855651855 sec\n",
            "\n",
            "Epoch 68 Batch 0 loss 1.2137418985366821\n",
            "Epoch 68 Loss 1.2137\n",
            "Time taken for 1 epoch 6.222156286239624 sec\n",
            "\n",
            "Epoch 69 Batch 0 loss 1.1309553384780884\n",
            "Epoch 69 Loss 1.1310\n",
            "Time taken for 1 epoch 0.8325185775756836 sec\n",
            "\n",
            "Epoch 70 Batch 0 loss 1.0590629577636719\n",
            "Epoch 70 Loss 1.0591\n",
            "Time taken for 1 epoch 1.143848180770874 sec\n",
            "\n",
            "Epoch 71 Batch 0 loss 1.1223574876785278\n",
            "Epoch 71 Loss 1.1224\n",
            "Time taken for 1 epoch 0.7749388217926025 sec\n",
            "\n",
            "Epoch 72 Batch 0 loss 1.0190826654434204\n",
            "Epoch 72 Loss 1.0191\n",
            "Time taken for 1 epoch 4.932354211807251 sec\n",
            "\n",
            "Epoch 73 Batch 0 loss 0.9980766177177429\n",
            "Epoch 73 Loss 0.9981\n",
            "Time taken for 1 epoch 0.8574309349060059 sec\n",
            "\n",
            "Epoch 74 Batch 0 loss 1.002966046333313\n",
            "Epoch 74 Loss 1.0030\n",
            "Time taken for 1 epoch 1.1674377918243408 sec\n",
            "\n",
            "Epoch 75 Batch 0 loss 0.959653913974762\n",
            "Epoch 75 Loss 0.9597\n",
            "Time taken for 1 epoch 0.7733414173126221 sec\n",
            "\n",
            "Epoch 76 Batch 0 loss 0.955899715423584\n",
            "Epoch 76 Loss 0.9559\n",
            "Time taken for 1 epoch 1.2111995220184326 sec\n",
            "\n",
            "Epoch 77 Batch 0 loss 0.8793075680732727\n",
            "Epoch 77 Loss 0.8793\n",
            "Time taken for 1 epoch 0.7700436115264893 sec\n",
            "\n",
            "Epoch 78 Batch 0 loss 0.9000658988952637\n",
            "Epoch 78 Loss 0.9001\n",
            "Time taken for 1 epoch 2.3569655418395996 sec\n",
            "\n",
            "Epoch 79 Batch 0 loss 0.9009769558906555\n",
            "Epoch 79 Loss 0.9010\n",
            "Time taken for 1 epoch 0.7747094631195068 sec\n",
            "\n",
            "Epoch 80 Batch 0 loss 0.9526757001876831\n",
            "Epoch 80 Loss 0.9527\n",
            "Time taken for 1 epoch 3.7356173992156982 sec\n",
            "\n",
            "Epoch 81 Batch 0 loss 0.8469253778457642\n",
            "Epoch 81 Loss 0.8469\n",
            "Time taken for 1 epoch 0.8361594676971436 sec\n",
            "\n",
            "Epoch 82 Batch 0 loss 0.9282107949256897\n",
            "Epoch 82 Loss 0.9282\n",
            "Time taken for 1 epoch 5.554210424423218 sec\n",
            "\n",
            "Epoch 83 Batch 0 loss 0.7945968508720398\n",
            "Epoch 83 Loss 0.7946\n",
            "Time taken for 1 epoch 0.838019847869873 sec\n",
            "\n",
            "Epoch 84 Batch 0 loss 0.8616589903831482\n",
            "Epoch 84 Loss 0.8617\n",
            "Time taken for 1 epoch 1.2208361625671387 sec\n",
            "\n",
            "Epoch 85 Batch 0 loss 0.7445948123931885\n",
            "Epoch 85 Loss 0.7446\n",
            "Time taken for 1 epoch 0.7697205543518066 sec\n",
            "\n",
            "Epoch 86 Batch 0 loss 0.777671754360199\n",
            "Epoch 86 Loss 0.7777\n",
            "Time taken for 1 epoch 1.8092451095581055 sec\n",
            "\n",
            "Epoch 87 Batch 0 loss 0.8062116503715515\n",
            "Epoch 87 Loss 0.8062\n",
            "Time taken for 1 epoch 0.7776012420654297 sec\n",
            "\n",
            "Epoch 88 Batch 0 loss 0.7458519339561462\n",
            "Epoch 88 Loss 0.7459\n",
            "Time taken for 1 epoch 2.525062084197998 sec\n",
            "\n",
            "Epoch 89 Batch 0 loss 0.7462608218193054\n",
            "Epoch 89 Loss 0.7463\n",
            "Time taken for 1 epoch 0.7729904651641846 sec\n",
            "\n",
            "Epoch 90 Batch 0 loss 0.7299277186393738\n",
            "Epoch 90 Loss 0.7299\n",
            "Time taken for 1 epoch 6.14698052406311 sec\n",
            "\n",
            "Epoch 91 Batch 0 loss 0.7087051272392273\n",
            "Epoch 91 Loss 0.7087\n",
            "Time taken for 1 epoch 0.8591368198394775 sec\n",
            "\n",
            "Epoch 92 Batch 0 loss 0.6698386669158936\n",
            "Epoch 92 Loss 0.6698\n",
            "Time taken for 1 epoch 1.1552484035491943 sec\n",
            "\n",
            "Epoch 93 Batch 0 loss 0.6315239667892456\n",
            "Epoch 93 Loss 0.6315\n",
            "Time taken for 1 epoch 0.7776377201080322 sec\n",
            "\n",
            "Epoch 94 Batch 0 loss 0.6728808879852295\n",
            "Epoch 94 Loss 0.6729\n",
            "Time taken for 1 epoch 1.7184686660766602 sec\n",
            "\n",
            "Epoch 95 Batch 0 loss 0.6398111581802368\n",
            "Epoch 95 Loss 0.6398\n",
            "Time taken for 1 epoch 0.7815840244293213 sec\n",
            "\n",
            "Epoch 96 Batch 0 loss 0.6471293568611145\n",
            "Epoch 96 Loss 0.6471\n",
            "Time taken for 1 epoch 3.4629900455474854 sec\n",
            "\n",
            "Epoch 97 Batch 0 loss 0.6421611309051514\n",
            "Epoch 97 Loss 0.6422\n",
            "Time taken for 1 epoch 0.8342282772064209 sec\n",
            "\n",
            "Epoch 98 Batch 0 loss 0.6244667172431946\n",
            "Epoch 98 Loss 0.6245\n",
            "Time taken for 1 epoch 2.7245113849639893 sec\n",
            "\n",
            "Epoch 99 Batch 0 loss 0.648909330368042\n",
            "Epoch 99 Loss 0.6489\n",
            "Time taken for 1 epoch 0.8572921752929688 sec\n",
            "\n",
            "Epoch 100 Batch 0 loss 0.6187378168106079\n",
            "Epoch 100 Loss 0.6187\n",
            "Time taken for 1 epoch 5.932683944702148 sec\n",
            "\n",
            "Epoch 101 Batch 0 loss 0.5916557908058167\n",
            "Epoch 101 Loss 0.5917\n",
            "Time taken for 1 epoch 0.8691177368164062 sec\n",
            "\n",
            "Epoch 102 Batch 0 loss 0.614902138710022\n",
            "Epoch 102 Loss 0.6149\n",
            "Time taken for 1 epoch 1.1485064029693604 sec\n",
            "\n",
            "Epoch 103 Batch 0 loss 0.5732401013374329\n",
            "Epoch 103 Loss 0.5732\n",
            "Time taken for 1 epoch 0.7860927581787109 sec\n",
            "\n",
            "Epoch 104 Batch 0 loss 0.5803656578063965\n",
            "Epoch 104 Loss 0.5804\n",
            "Time taken for 1 epoch 2.2262260913848877 sec\n",
            "\n",
            "Epoch 105 Batch 0 loss 0.5742946863174438\n",
            "Epoch 105 Loss 0.5743\n",
            "Time taken for 1 epoch 0.7839329242706299 sec\n",
            "\n",
            "Epoch 106 Batch 0 loss 0.5753803849220276\n",
            "Epoch 106 Loss 0.5754\n",
            "Time taken for 1 epoch 2.4945123195648193 sec\n",
            "\n",
            "Epoch 107 Batch 0 loss 0.5513631105422974\n",
            "Epoch 107 Loss 0.5514\n",
            "Time taken for 1 epoch 0.7835288047790527 sec\n",
            "\n",
            "Epoch 108 Batch 0 loss 0.49984094500541687\n",
            "Epoch 108 Loss 0.4998\n",
            "Time taken for 1 epoch 6.132597923278809 sec\n",
            "\n",
            "Epoch 109 Batch 0 loss 0.532370388507843\n",
            "Epoch 109 Loss 0.5324\n",
            "Time taken for 1 epoch 0.8462738990783691 sec\n",
            "\n",
            "Epoch 110 Batch 0 loss 0.5443368554115295\n",
            "Epoch 110 Loss 0.5443\n",
            "Time taken for 1 epoch 1.1270909309387207 sec\n",
            "\n",
            "Epoch 111 Batch 0 loss 0.5213620066642761\n",
            "Epoch 111 Loss 0.5214\n",
            "Time taken for 1 epoch 0.7854456901550293 sec\n",
            "\n",
            "Epoch 112 Batch 0 loss 0.5237202644348145\n",
            "Epoch 112 Loss 0.5237\n",
            "Time taken for 1 epoch 1.1898260116577148 sec\n",
            "\n",
            "Epoch 113 Batch 0 loss 0.5005937814712524\n",
            "Epoch 113 Loss 0.5006\n",
            "Time taken for 1 epoch 0.7797806262969971 sec\n",
            "\n",
            "Epoch 114 Batch 0 loss 0.4997216463088989\n",
            "Epoch 114 Loss 0.4997\n",
            "Time taken for 1 epoch 6.561866283416748 sec\n",
            "\n",
            "Epoch 115 Batch 0 loss 0.5276753902435303\n",
            "Epoch 115 Loss 0.5277\n",
            "Time taken for 1 epoch 0.8597111701965332 sec\n",
            "\n",
            "Epoch 116 Batch 0 loss 0.4941602349281311\n",
            "Epoch 116 Loss 0.4942\n",
            "Time taken for 1 epoch 1.164647102355957 sec\n",
            "\n",
            "Epoch 117 Batch 0 loss 0.5237805843353271\n",
            "Epoch 117 Loss 0.5238\n",
            "Time taken for 1 epoch 0.7833781242370605 sec\n",
            "\n",
            "Epoch 118 Batch 0 loss 0.5242749452590942\n",
            "Epoch 118 Loss 0.5243\n",
            "Time taken for 1 epoch 1.171294927597046 sec\n",
            "\n",
            "Epoch 119 Batch 0 loss 0.49971288442611694\n",
            "Epoch 119 Loss 0.4997\n",
            "Time taken for 1 epoch 0.7895917892456055 sec\n",
            "\n",
            "Epoch 120 Batch 0 loss 0.4868898391723633\n",
            "Epoch 120 Loss 0.4869\n",
            "Time taken for 1 epoch 3.172699451446533 sec\n",
            "\n",
            "Epoch 121 Batch 0 loss 0.4841834008693695\n",
            "Epoch 121 Loss 0.4842\n",
            "Time taken for 1 epoch 0.8625054359436035 sec\n",
            "\n",
            "Epoch 122 Batch 0 loss 0.47055816650390625\n",
            "Epoch 122 Loss 0.4706\n",
            "Time taken for 1 epoch 2.0566134452819824 sec\n",
            "\n",
            "Epoch 123 Batch 0 loss 0.4852113425731659\n",
            "Epoch 123 Loss 0.4852\n",
            "Time taken for 1 epoch 0.7950725555419922 sec\n",
            "\n",
            "Epoch 124 Batch 0 loss 0.4819985330104828\n",
            "Epoch 124 Loss 0.4820\n",
            "Time taken for 1 epoch 4.210846900939941 sec\n",
            "\n",
            "Epoch 125 Batch 0 loss 0.48901015520095825\n",
            "Epoch 125 Loss 0.4890\n",
            "Time taken for 1 epoch 0.8461666107177734 sec\n",
            "\n",
            "Epoch 126 Batch 0 loss 0.49273014068603516\n",
            "Epoch 126 Loss 0.4927\n",
            "Time taken for 1 epoch 1.897092342376709 sec\n",
            "\n",
            "Epoch 127 Batch 0 loss 0.49845001101493835\n",
            "Epoch 127 Loss 0.4985\n",
            "Time taken for 1 epoch 0.7896671295166016 sec\n",
            "\n",
            "Epoch 128 Batch 0 loss 0.4714142382144928\n",
            "Epoch 128 Loss 0.4714\n",
            "Time taken for 1 epoch 5.929838180541992 sec\n",
            "\n",
            "Epoch 129 Batch 0 loss 0.45584800839424133\n",
            "Epoch 129 Loss 0.4558\n",
            "Time taken for 1 epoch 0.8406515121459961 sec\n",
            "\n",
            "Epoch 130 Batch 0 loss 0.4642893970012665\n",
            "Epoch 130 Loss 0.4643\n",
            "Time taken for 1 epoch 1.1439473628997803 sec\n",
            "\n",
            "Epoch 131 Batch 0 loss 0.472638875246048\n",
            "Epoch 131 Loss 0.4726\n",
            "Time taken for 1 epoch 0.7849833965301514 sec\n",
            "\n",
            "Epoch 132 Batch 0 loss 0.4497182369232178\n",
            "Epoch 132 Loss 0.4497\n",
            "Time taken for 1 epoch 1.8855085372924805 sec\n",
            "\n",
            "Epoch 133 Batch 0 loss 0.42585188150405884\n",
            "Epoch 133 Loss 0.4259\n",
            "Time taken for 1 epoch 0.7900211811065674 sec\n",
            "\n",
            "Epoch 134 Batch 0 loss 0.45447301864624023\n",
            "Epoch 134 Loss 0.4545\n",
            "Time taken for 1 epoch 2.5124385356903076 sec\n",
            "\n",
            "Epoch 135 Batch 0 loss 0.43926647305488586\n",
            "Epoch 135 Loss 0.4393\n",
            "Time taken for 1 epoch 0.7883455753326416 sec\n",
            "\n",
            "Epoch 136 Batch 0 loss 0.48542723059654236\n",
            "Epoch 136 Loss 0.4854\n",
            "Time taken for 1 epoch 6.125316381454468 sec\n",
            "\n",
            "Epoch 137 Batch 0 loss 0.4676182270050049\n",
            "Epoch 137 Loss 0.4676\n",
            "Time taken for 1 epoch 0.8692796230316162 sec\n",
            "\n",
            "Epoch 138 Batch 0 loss 0.4524672031402588\n",
            "Epoch 138 Loss 0.4525\n",
            "Time taken for 1 epoch 1.1747241020202637 sec\n",
            "\n",
            "Epoch 139 Batch 0 loss 0.44450294971466064\n",
            "Epoch 139 Loss 0.4445\n",
            "Time taken for 1 epoch 0.7963464260101318 sec\n",
            "\n",
            "Epoch 140 Batch 0 loss 0.43361949920654297\n",
            "Epoch 140 Loss 0.4336\n",
            "Time taken for 1 epoch 1.7397513389587402 sec\n",
            "\n",
            "Epoch 141 Batch 0 loss 0.46656668186187744\n",
            "Epoch 141 Loss 0.4666\n",
            "Time taken for 1 epoch 0.7951793670654297 sec\n",
            "\n",
            "Epoch 142 Batch 0 loss 0.45513105392456055\n",
            "Epoch 142 Loss 0.4551\n",
            "Time taken for 1 epoch 3.564199686050415 sec\n",
            "\n",
            "Epoch 143 Batch 0 loss 0.4146350026130676\n",
            "Epoch 143 Loss 0.4146\n",
            "Time taken for 1 epoch 0.8516736030578613 sec\n",
            "\n",
            "Epoch 144 Batch 0 loss 0.4417327344417572\n",
            "Epoch 144 Loss 0.4417\n",
            "Time taken for 1 epoch 2.166682481765747 sec\n",
            "\n",
            "Epoch 145 Batch 0 loss 0.4243566691875458\n",
            "Epoch 145 Loss 0.4244\n",
            "Time taken for 1 epoch 0.7925417423248291 sec\n",
            "\n",
            "Epoch 146 Batch 0 loss 0.41863393783569336\n",
            "Epoch 146 Loss 0.4186\n",
            "Time taken for 1 epoch 6.772258758544922 sec\n",
            "\n",
            "Epoch 147 Batch 0 loss 0.42269086837768555\n",
            "Epoch 147 Loss 0.4227\n",
            "Time taken for 1 epoch 0.8524057865142822 sec\n",
            "\n",
            "Epoch 148 Batch 0 loss 0.41760915517807007\n",
            "Epoch 148 Loss 0.4176\n",
            "Time taken for 1 epoch 1.179520606994629 sec\n",
            "\n",
            "Epoch 149 Batch 0 loss 0.4181773364543915\n",
            "Epoch 149 Loss 0.4182\n",
            "Time taken for 1 epoch 0.7923984527587891 sec\n",
            "\n",
            "Epoch 150 Batch 0 loss 0.4004572927951813\n",
            "Epoch 150 Loss 0.4005\n",
            "Time taken for 1 epoch 1.2067785263061523 sec\n",
            "\n",
            "Epoch 151 Batch 0 loss 0.4118463695049286\n",
            "Epoch 151 Loss 0.4118\n",
            "Time taken for 1 epoch 0.7945659160614014 sec\n",
            "\n",
            "Epoch 152 Batch 0 loss 0.4169471561908722\n",
            "Epoch 152 Loss 0.4169\n",
            "Time taken for 1 epoch 2.9280242919921875 sec\n",
            "\n",
            "Epoch 153 Batch 0 loss 0.42854586243629456\n",
            "Epoch 153 Loss 0.4285\n",
            "Time taken for 1 epoch 0.8714375495910645 sec\n",
            "\n",
            "Epoch 154 Batch 0 loss 0.42621877789497375\n",
            "Epoch 154 Loss 0.4262\n",
            "Time taken for 1 epoch 3.150409460067749 sec\n",
            "\n",
            "Epoch 155 Batch 0 loss 0.42652666568756104\n",
            "Epoch 155 Loss 0.4265\n",
            "Time taken for 1 epoch 0.866283655166626 sec\n",
            "\n",
            "Epoch 156 Batch 0 loss 0.37327608466148376\n",
            "Epoch 156 Loss 0.3733\n",
            "Time taken for 1 epoch 5.306166887283325 sec\n",
            "\n",
            "Epoch 157 Batch 0 loss 0.3899240791797638\n",
            "Epoch 157 Loss 0.3899\n",
            "Time taken for 1 epoch 0.8598134517669678 sec\n",
            "\n",
            "Epoch 158 Batch 0 loss 0.4004800319671631\n",
            "Epoch 158 Loss 0.4005\n",
            "Time taken for 1 epoch 1.1982390880584717 sec\n",
            "\n",
            "Epoch 159 Batch 0 loss 0.40496280789375305\n",
            "Epoch 159 Loss 0.4050\n",
            "Time taken for 1 epoch 0.7931430339813232 sec\n",
            "\n",
            "Epoch 160 Batch 0 loss 0.3804472088813782\n",
            "Epoch 160 Loss 0.3804\n",
            "Time taken for 1 epoch 1.749708890914917 sec\n",
            "\n",
            "Epoch 161 Batch 0 loss 0.3895934224128723\n",
            "Epoch 161 Loss 0.3896\n",
            "Time taken for 1 epoch 0.7898023128509521 sec\n",
            "\n",
            "Epoch 162 Batch 0 loss 0.38766729831695557\n",
            "Epoch 162 Loss 0.3877\n",
            "Time taken for 1 epoch 5.643719434738159 sec\n",
            "\n",
            "Epoch 163 Batch 0 loss 0.3554496169090271\n",
            "Epoch 163 Loss 0.3554\n",
            "Time taken for 1 epoch 0.8641741275787354 sec\n",
            "\n",
            "Epoch 164 Batch 0 loss 0.3560488224029541\n",
            "Epoch 164 Loss 0.3560\n",
            "Time taken for 1 epoch 1.1774790287017822 sec\n",
            "\n",
            "Epoch 165 Batch 0 loss 0.3769359290599823\n",
            "Epoch 165 Loss 0.3769\n",
            "Time taken for 1 epoch 0.7906816005706787 sec\n",
            "\n",
            "Epoch 166 Batch 0 loss 0.36824876070022583\n",
            "Epoch 166 Loss 0.3682\n",
            "Time taken for 1 epoch 1.6074028015136719 sec\n",
            "\n",
            "Epoch 167 Batch 0 loss 0.34732502698898315\n",
            "Epoch 167 Loss 0.3473\n",
            "Time taken for 1 epoch 0.7940211296081543 sec\n",
            "\n",
            "Epoch 168 Batch 0 loss 0.3339863419532776\n",
            "Epoch 168 Loss 0.3340\n",
            "Time taken for 1 epoch 3.4271953105926514 sec\n",
            "\n",
            "Epoch 169 Batch 0 loss 0.3655409514904022\n",
            "Epoch 169 Loss 0.3655\n",
            "Time taken for 1 epoch 0.8685753345489502 sec\n",
            "\n",
            "Epoch 170 Batch 0 loss 0.3201185464859009\n",
            "Epoch 170 Loss 0.3201\n",
            "Time taken for 1 epoch 3.196427822113037 sec\n",
            "\n",
            "Epoch 171 Batch 0 loss 0.3825300931930542\n",
            "Epoch 171 Loss 0.3825\n",
            "Time taken for 1 epoch 0.8542354106903076 sec\n",
            "\n",
            "Epoch 172 Batch 0 loss 0.35309815406799316\n",
            "Epoch 172 Loss 0.3531\n",
            "Time taken for 1 epoch 2.4457736015319824 sec\n",
            "\n",
            "Epoch 173 Batch 0 loss 0.34717392921447754\n",
            "Epoch 173 Loss 0.3472\n",
            "Time taken for 1 epoch 0.7955305576324463 sec\n",
            "\n",
            "Epoch 174 Batch 0 loss 0.35079991817474365\n",
            "Epoch 174 Loss 0.3508\n",
            "Time taken for 1 epoch 3.1552014350891113 sec\n",
            "\n",
            "Epoch 175 Batch 0 loss 0.34318849444389343\n",
            "Epoch 175 Loss 0.3432\n",
            "Time taken for 1 epoch 0.8518686294555664 sec\n",
            "\n",
            "Epoch 176 Batch 0 loss 0.303405225276947\n",
            "Epoch 176 Loss 0.3034\n",
            "Time taken for 1 epoch 2.018573045730591 sec\n",
            "\n",
            "Epoch 177 Batch 0 loss 0.3528655767440796\n",
            "Epoch 177 Loss 0.3529\n",
            "Time taken for 1 epoch 0.7933022975921631 sec\n",
            "\n",
            "Epoch 178 Batch 0 loss 0.3172464072704315\n",
            "Epoch 178 Loss 0.3172\n",
            "Time taken for 1 epoch 6.393080711364746 sec\n",
            "\n",
            "Epoch 179 Batch 0 loss 0.3430897891521454\n",
            "Epoch 179 Loss 0.3431\n",
            "Time taken for 1 epoch 0.8541684150695801 sec\n",
            "\n",
            "Epoch 180 Batch 0 loss 0.34877461194992065\n",
            "Epoch 180 Loss 0.3488\n",
            "Time taken for 1 epoch 1.1999311447143555 sec\n",
            "\n",
            "Epoch 181 Batch 0 loss 0.3014203608036041\n",
            "Epoch 181 Loss 0.3014\n",
            "Time taken for 1 epoch 0.7866454124450684 sec\n",
            "\n",
            "Epoch 182 Batch 0 loss 0.33377835154533386\n",
            "Epoch 182 Loss 0.3338\n",
            "Time taken for 1 epoch 1.7637593746185303 sec\n",
            "\n",
            "Epoch 183 Batch 0 loss 0.3099532723426819\n",
            "Epoch 183 Loss 0.3100\n",
            "Time taken for 1 epoch 0.793757438659668 sec\n",
            "\n",
            "Epoch 184 Batch 0 loss 0.3133624494075775\n",
            "Epoch 184 Loss 0.3134\n",
            "Time taken for 1 epoch 3.0311806201934814 sec\n",
            "\n",
            "Epoch 185 Batch 0 loss 0.3191705644130707\n",
            "Epoch 185 Loss 0.3192\n",
            "Time taken for 1 epoch 0.868232011795044 sec\n",
            "\n",
            "Epoch 186 Batch 0 loss 0.32576701045036316\n",
            "Epoch 186 Loss 0.3258\n",
            "Time taken for 1 epoch 2.76212739944458 sec\n",
            "\n",
            "Epoch 187 Batch 0 loss 0.32816916704177856\n",
            "Epoch 187 Loss 0.3282\n",
            "Time taken for 1 epoch 0.865649938583374 sec\n",
            "\n",
            "Epoch 188 Batch 0 loss 0.3946310579776764\n",
            "Epoch 188 Loss 0.3946\n",
            "Time taken for 1 epoch 5.906216144561768 sec\n",
            "\n",
            "Epoch 189 Batch 0 loss 0.3855975568294525\n",
            "Epoch 189 Loss 0.3856\n",
            "Time taken for 1 epoch 0.88224196434021 sec\n",
            "\n",
            "Epoch 190 Batch 0 loss 0.6029987931251526\n",
            "Epoch 190 Loss 0.6030\n",
            "Time taken for 1 epoch 1.1865527629852295 sec\n",
            "\n",
            "Epoch 191 Batch 0 loss 0.3883083462715149\n",
            "Epoch 191 Loss 0.3883\n",
            "Time taken for 1 epoch 0.7924540042877197 sec\n",
            "\n",
            "Epoch 192 Batch 0 loss 0.36303243041038513\n",
            "Epoch 192 Loss 0.3630\n",
            "Time taken for 1 epoch 1.6352369785308838 sec\n",
            "\n",
            "Epoch 193 Batch 0 loss 0.4166696071624756\n",
            "Epoch 193 Loss 0.4167\n",
            "Time taken for 1 epoch 0.7903594970703125 sec\n",
            "\n",
            "Epoch 194 Batch 0 loss 0.3854004442691803\n",
            "Epoch 194 Loss 0.3854\n",
            "Time taken for 1 epoch 3.2621052265167236 sec\n",
            "\n",
            "Epoch 195 Batch 0 loss 0.3654868006706238\n",
            "Epoch 195 Loss 0.3655\n",
            "Time taken for 1 epoch 0.8562836647033691 sec\n",
            "\n",
            "Epoch 196 Batch 0 loss 0.3903501331806183\n",
            "Epoch 196 Loss 0.3904\n",
            "Time taken for 1 epoch 2.0094618797302246 sec\n",
            "\n",
            "Epoch 197 Batch 0 loss 0.3628219664096832\n",
            "Epoch 197 Loss 0.3628\n",
            "Time taken for 1 epoch 0.792412519454956 sec\n",
            "\n",
            "Epoch 198 Batch 0 loss 0.36014243960380554\n",
            "Epoch 198 Loss 0.3601\n",
            "Time taken for 1 epoch 3.519031524658203 sec\n",
            "\n",
            "Epoch 199 Batch 0 loss 0.3755697011947632\n",
            "Epoch 199 Loss 0.3756\n",
            "Time taken for 1 epoch 0.8779432773590088 sec\n",
            "\n",
            "Epoch 200 Batch 0 loss 0.35065340995788574\n",
            "Epoch 200 Loss 0.3507\n",
            "Time taken for 1 epoch 3.2706778049468994 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_target_length, max_source_length))\n",
        "\n",
        "  sentence = 'start_ ' + sentence + ' _end'\n",
        "  #print(sentence)\n",
        "  #print(source_sentence_tokenizer.word_index)\n",
        "\n",
        "  inputs = [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_source_length,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
        "\n",
        "  for t in range(max_target_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtqyfy2Yos-q"
      },
      "source": [
        "## To Plot Attention weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  \n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2272e51e-25e8-4a79-bc9d-2a148226338d"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3dfda27278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIfMV09Los-r"
      },
      "source": [
        "## Final translations with Attention Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIhgPd4mos-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63cd736-0998-4bdb-a00b-b4551ed1d34e"
      },
      "source": [
        "translate('американдық авторлар арасында')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: start_ американдық авторлар арасында _end\n",
            "Predicted translation: outcome _end \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYxv_bFEos-s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}